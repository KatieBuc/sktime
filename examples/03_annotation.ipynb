{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "\n",
    "In this notebook, we'll explore how to annotate various data sets. Annotation broadly refers to a range of techniques spanning from anomaly/change point detection to segmentation. Such techniques are used in a diverse range of applications, for instance, the identification of extreme weather events, stock market crashes and fraudulent transactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sktime.annotation.datagen import piecewise_normal_multivariate\n",
    "from sktime.utils.validation._dependencies import _check_soft_dependencies\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "_check_soft_dependencies(\"ipympl\", severity=\"warning\")\n",
    "\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll generate some multivariate data that exhibits mean shifts (with constant covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_mean = piecewise_normal_multivariate(\n",
    "    means=[[0, 3], [5, 0], [10, 3]],\n",
    "    lengths=[30, 50, 20],\n",
    "    covariances=[\n",
    "        [[0.5, 0.3], [0.3, 1.0]],\n",
    "        [[0.5, 0.3], [0.3, 1.0]],\n",
    "        [[0.5, 0.3], [0.3, 1.0]],\n",
    "    ],\n",
    "    random_state=279,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(mv_mean)\n",
    "ax.legend([\"y_0\", \"y_1\"])\n",
    "\n",
    "plt.title(\"Mean-shifted multivariate timeseries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data space, this can be represented as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(mv_mean[:, 0], mv_mean[:, 1], \"o\")\n",
    "\n",
    "plt.title(\"Mean-shifted multivariate data\")\n",
    "plt.xlabel(\"y_0\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we generate a multivariate timeseires by changing the covariance structure (from independent random variables, to highly negatively correlated, to highly correlated). This can be seen in the covariances matrices input below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_cov = piecewise_normal_multivariate(\n",
    "    means=[[1.1, 3], [1, 2.9], [1, 3]],\n",
    "    lengths=[40, 50, 60],\n",
    "    covariances=[\n",
    "        [[1.0, 0.003], [0.003, 1.0]],\n",
    "        [[1.0, -0.999], [-0.999, 1.0]],\n",
    "        [[1.0, 1.0], [1.0, 1.0]],\n",
    "    ],\n",
    "    random_state=2354,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(mv_cov)\n",
    "\n",
    "ax.legend([\"y_0\", \"y_1\"])\n",
    "\n",
    "plt.title(\"Covariance-shifted multivariate timeseries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representing this timeseries with changing covariace in the data space allows us to uncover interesting characteristics; the 45 degree angle line comes about from the phase of high correlation and the 135 degree line comes about from the phase of negative correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(mv_cov[:, 0], mv_cov[:, 1], \"o\")\n",
    "\n",
    "plt.title(\"Covariance-shifted multivariate data\")\n",
    "plt.xlabel(\"y_0\")\n",
    "plt.ylabel(\"y_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different definitions of \"dimension\": we use a 2-dimensional array to represent tabular data, but also describe the dimensionality of data (in the data space above, which is the number of columns). Intrinsic dimensionality is something else entirely..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to understand phases and/or changing complex system regimes (with potentially hidden states). We will now explore different techniques to segment the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMM: Hidden Markov Model\n",
    "\n",
    "A hidden Markov model (HMM) allows us to account for both observed events and hidden events that we think of as causal factors in our probabilistic model. This is done by fitting a sequence of hidden states to a sequence of observations, by finding the most likely path given the emission probabilities - (i.e. the probability that a particular observation\n",
    "would be generated by a given hidden state), the transition probability (i.e. the probability of transitioning from one state to another or staying in the same state) and the initial probabilities - i.e. the belief of the probability distribution of hidden states at the\n",
    "start of the observation sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.annotation.hmm_learn import GMMHMM\n",
    "\n",
    "X = mv_mean[:, 0]\n",
    "model = GMMHMM(n_components=3, random_state=123)\n",
    "model = model.fit(X)\n",
    "labels = model.predict(X)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={\"height_ratios\": [5, 1]})\n",
    "axs[0].plot(X)\n",
    "axs[1].plot(labels, \"o\")\n",
    "\n",
    "fig.suptitle(\"HMM change point detection of univariate timeseries (mean-shifted)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIDALGO: Heterogeneous Intrinsic Dimensionality ALGOrithm\n",
    "\n",
    "A fundamental paradigm in machine learning is that a small number of variables is often sufficient to describe high-dimensional data, where Intrinsic dimensionality (ID) is the minimum number of variables required. In most approaches for dimensionality reduction and manifold learning, the ID is assumed to be constant in the dataset. However, this assumption is often violated in the case of real-world data. Heterogeneous ID allows for data to be represented by a mixture model on the support on the union of K manifolds with varying dimensions. Points are then assigned to manifold k with corresponding dimensionality. Hidalgo extends this concept to a Bayesian framework which allows the identification (by Gibbs sampling) of regions in the data landscape where the ID can be considered constant. Hidalgo offers robust method for unsupervised segmentation of high-dimensional data [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.hidalgo import Hidalgo\n",
    "\n",
    "model = Hidalgo(K=3, burn_in=0.8, n_iter=100, seed=10)\n",
    "fitted_model = model.fit(mv_mean)\n",
    "z_mv_mean = fitted_model.transform(mv_mean)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={\"height_ratios\": [5, 1]})\n",
    "axs[0].plot(mv_mean)\n",
    "axs[1].plot(z_mv_mean, \"o\")\n",
    "\n",
    "fig.suptitle(\"HIDALGO segmentation of multivariate timeseries (mean-shifted)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Hidalgo(K=2, burn_in=0.8, n_iter=1000, seed=148)\n",
    "fitted_model = model.fit(mv_cov)\n",
    "z_mv_cov = fitted_model.transform(mv_cov)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={\"height_ratios\": [5, 1]})\n",
    "axs[0].plot(mv_cov)\n",
    "axs[1].plot(z_mv_cov, \"o\")\n",
    "\n",
    "fig.suptitle(\"HIDALGO segmentation of multivariate timeseries (covariance-shifted)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STRAY: Search TRace AnomalY outlier estimator\n",
    "\n",
    "STRAY [2] is an extension of HDoutliers [3], a powerful algorithm for the detection of anomalous observations in a dataset which has (among other advantages) the ability to detect clusters of outliers in multi-dimensional data without requiring a model of the typical behavior of the system. However, it suffers from some limitations that affect its accuracy. STRAY is an extension of HDoutliers that uses extreme value theory for the anomalous threshold calculation, to deal with data streams that exhibit non-stationary behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sktime.annotation.stray import STRAY\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(mv_mean)\n",
    "model = STRAY(alpha=0.1)\n",
    "y_actual = model.fit_transform(X)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={\"height_ratios\": [5, 1]})\n",
    "axs[0].plot(X)\n",
    "axs[1].plot(y_actual, \"o\")\n",
    "\n",
    "fig.suptitle(\"Anomaly detection for multivariate timeseries (mean-shifted)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(mv_mean)\n",
    "X[10][0] = 1.2\n",
    "X[10][1] = -0.8\n",
    "\n",
    "model = STRAY(alpha=0.1)\n",
    "y_actual = model.fit_transform(X)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={\"height_ratios\": [5, 1]})\n",
    "axs[0].plot(X)\n",
    "axs[1].plot(y_actual, \"o\")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Anomaly detection for multivariate timeseries with outlier (mean-shifted)\"\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Pipelines\n",
    "\n",
    "`sktime` has a range of pre-processing transformations available for use, to build analysis pipelines. There is a partcular method; a dimensionality reductuion technique specifically for the purpose of anomaly detection tasks (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DOBIN: Distance based Outlier BasIs using Neighbors\n",
    "\n",
    "A common challenge in many domains is that data is high-dimensional. Principal Component Analysis (PCA) is often used for dimension reduction when detecting outliers, however PCA finds a set of basis vectors that explains the variance of the data, such that the highest variance is in the direction of the first vector, and so on. But an outlier in the original high dimensional space might no longer be an outlier in the low dimesional subspace created by the dimension reduction method. So, outliers can be lost in this pre-processing step. DOBIN [4] constructs a set of basis vectors specifically tailored for unsupervised outlier detection to be used as a dimension reduction tool for outlier detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_mean_outlier = piecewise_normal_multivariate(\n",
    "    means=[[-2, -2, 0], [2, 2, 0], [3, 3, 1], [0, -10, 0]],\n",
    "    lengths=[30, 50, 20, 3],\n",
    "    random_state=279,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(mv_mean_outlier)\n",
    "\n",
    "ax.legend([\"y_0\", \"y_1\", \"y_2\"])\n",
    "\n",
    "plt.title(\"Mean-shifted multivariate timeseries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.plot(mv_mean_outlier[:, 0], mv_mean_outlier[:, 1], mv_mean_outlier[:, 2], \"o\")\n",
    "ax.set_xlabel(\"y_0\")\n",
    "ax.set_ylabel(\"y_1\")\n",
    "ax.set_zlabel(\"y_2\")\n",
    "\n",
    "plt.title(\"Mean-shifted multivariate data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(mv_mean_outlier)\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(X_pca[:, 0], X_pca[:, 1], \"o\")\n",
    "ax.plot(X_pca[100:, 0], X_pca[100:, 1], \"o\", \"r\")\n",
    "\n",
    "ax.set_xlabel(\"Princical Component 0\")\n",
    "ax.set_ylabel(\"Principal Component 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sktime.transformations.series.dobin import DOBIN\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(mv_mean_outlier)\n",
    "model = DOBIN()\n",
    "X_outlier = model.fit_transform(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.plot(X_outlier[\"DB0\"], X_outlier[\"DB1\"], \"o\")\n",
    "ax.plot(X_outlier[\"DB0\"][100:], X_outlier[\"DB1\"][100:], \"o\", \"r\")\n",
    "\n",
    "ax.set_xlabel(\"DOBIN BASIS 0\")\n",
    "ax.set_ylabel(\"DOBIN BASIS 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### References:\n",
    "[1] Allegra, Michele, et al. \"Data segmentation based on the local\n",
    "    intrinsic dimension.\" Scientific reports 10.1 (2020): 1-12.\n",
    "    https://www.nature.com/articles/s41598-020-72222-0\n",
    "\n",
    "[2] Talagala, Priyanga Dilini, Rob J. Hyndman, and Kate Smith-Miles.\n",
    "\"Anomaly detection in high-dimensional data.\" Journal of Computational\n",
    "and Graphical Statistics 30.2 (2021): 360-374.\n",
    "\n",
    "[3] Wilkinson, Leland. \"Visualizing big data outliers through\n",
    "distributed aggregation.\" IEEE transactions on visualization and\n",
    "computer graphics 24.1 (2017): 256-266.\n",
    "\n",
    "[4] Kandanaarachchi, Sevvandi, and Rob J. Hyndman. \"Dimension reduction\n",
    "    for outlier detection using DOBIN.\" Journal of Computational and Graphical\n",
    "    Statistics 30.1 (2021): 204-219.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Credits:\n",
    "\n",
    "sktime: https://github.com/sktime/sktime/blob/main/CONTRIBUTORS.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sktime-dev2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e978f459e3b866fb9e2311257b800312b505cfb40620ff4ff17f14dd43cc0ce1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
